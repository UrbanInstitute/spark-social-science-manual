# - Getting Started with Spark

The open source Apache Spark project has generously taken the time to support use in R and Python, far more accessible languages than what Spark itself is written in (Scala).

Below are links to tutorials written by the Urban Institute Research Programming team to help guide researchers through tasks typically performed by social scientists, but driven by Spark:

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; PySpark (Python):	https://github.com/UrbanInstitute/pyspark-tutorials

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SparkR (R):		https://github.com/UrbanInstitute/sparkr-tutorials

**Note**: The tutorials linked above assume that readers have some familiarity with Python and R, respectively.

## What About Stata and SAS?

Unfortunately, neither Stata nor SAS are supported by Spark. Stata and SAS are proprietary tools, so it's very unlikely they will ever be supported by the Spark project. While there is a framework for distributed computing in SAS, this software is very expensive and also requires constantly running clusters, raising the cost above consideration for most social science organizations.

## Choosing Between SparkR and PySpark

If you have a strong preference of language for R or Python, you should let that preference guide your decision. SparkR and PySpark are not dramatically different in terms of speed or available functionality (though we have learned that generalized linear modeling is more intuitive in SparkR).

* __Speed__: Each of the Spark language implementations (R, Python, Java and Scala) communicates with the Spark data type DataFrame, which is conceptually equivalent to a table in a relational database or a data.frame/DataFrame in R/Python. Spark DataFrames use Scala and Java (languages that compose core Spark) to optimize operations so that the execution speeds for standard data processing tasks written in any of these languages are functionally the same.
* __Available Functionality__: There is some difference in Spark 1.6 between SparkR and PySpark concerning the availability of core Spark functionality, with PySpark having greater functionality. However, Spark 2.0 significantly reduces the functionality gap between the two.

That being said, there are some notable differences that might affect your decision:

* __Statistical Modeling__: We have found SparkR to natively support statistical modeling with far more ease the PySpark. R's normal formula syntax works well, whereas PySpark's implementation has added complexity. If easy and familiar statistical modeling is important, you might want to stick with SparkR.
* __Code Syntax__: SparkR feels a bit more like normal R than PySpark feels like Python, although both have peculiarities. This is perhaps the case since R is natively built around data.frames, which are analogous to Spark DataFrames. For this reason, you might prefer SparkR if you’re equally competent at R and Python. Both of the tutorials linked above are oriented around the Spark DataFrame data type.
* __Open Source Development__: It seems like Python has the advantage here, in that more of the packages extending the functionality come written in or are accessible through PySpark than SparkR. This is likely a product of the current Spark community – more data scientists and engineers than traditional statisticians and social scientists.
