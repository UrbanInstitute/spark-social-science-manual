# - Data Visualization

## Visualization Process

While Spark is an excellent framework for the analysis of big data, it should be used purely as a data manipulation and analytical tool to enable visualization, rather than doing the visualization itself. Since R and Python both have powerful and extensive libraries for data visualization, there is a substantial advantage in creating graphics from local data on the master node.  

It's possible that these visualization tools will eventually be implemented directly into Spark, but unlikely that they will offer as thorough functionality as the non-distributed versions. By preparing data in Spark, then bringing 

For instance, the [ggplot2.SparkR](https://github.com/SKKU-SKT/ggplot2.SparkR) project is promising, and does offers a limited subset of R's ggplot2 geoms [directly in SparkR](https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/visualizations.md). However, the package has not been updated for eight months and the coverage of the graph types is not enough to be sufficent on its own.

Instead of relying on these packages, it is better to invest in understanding important graph types well enough to split the graphing process into a data processing step followed by a visualizing process. This chapter will further explain this process, and cover important graph types, with links to examples in the SparkR and PySpark tutorials.

## 



## Recommended Charts



Avoid graphs of equivalence - in which one observation becomes one visual mark on your graph.  

If you do decide to work with graphs of equivalence, keep in mind jittering and transparency as options to make more of your data meaningfully visible. 







