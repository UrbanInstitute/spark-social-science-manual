[
["index.html", "Spark Social Science Manual About Spark Social Science", " Spark Social Science Manual Research Programming, The Urban Institute Sarah Armstrong Alex C. Engler September 16, 2016 About Spark Social Science The Spark Social Science is an endeavor by the Urban Institute to expand access and understanding of modern big data capabilities to social scientists and public policy reseachers. There are four GitHub repositories associated with this project: - Spark Social Science – will walk you through the technical process of launching Spark in Amazon Web Services. - SparkR-Tutorials – contains a comprehensive introduction to working with Spark using R. - PySpark-Tutorials – contains a comprehensive introdution to working with Spark using Python. - Spark Social Science Manual – the GitHub Repository for this book. This work is generously funded by the Alfred P. Sloan Foundation. This work is licensed under the Gnu General Public License, Version 3, 29 June 2007. "],
["intro.html", "Chapter 1 - Introduction 1.1 Outgrowing the server: When should social scientists consider distributing computing? 1.2 Making distributed computing straightforward &amp; cost-effective", " Chapter 1 - Introduction Most social scientists perform statistical analysis on personal computers or on servers dedicated to statistical processing. While these environments do provide effective research platforms for the majority of projects, their inherent constraints on data size and processing speed limit the ability of social scientists to perform empirical analysis on large data sets. We begin this guide by discussing: alternative frameworks for data storage and performing statistical analysis, when social scientists may benefit from these approaches and the solution developed by Research Programming to address the massive data needs of policy researchers at The Urban Institute (UI). We subsequently provide an overview of the framework that supports the solution developed by Research Programming and best-practices for utilizing this solution. While understanding the mechanisms behind the framework in detail is not necessary to leverage the UI solution described below, a basic understanding will help researchers to perform their analyses more efficiently. 1.1 Outgrowing the server: When should social scientists consider distributing computing? When working with data on a personal computer, or accessing a server, the system pulls data from physical storage (your hard drive) and loads it into your system’s memory while you work with it (e.g. Stata), or iteratively reads the data into and out of memory (e.g. SAS). Whether you are working from your personal computer’s hard drive and memory, or utilizing a server, the size of the data being considered is confined to the constraints of that system. For the majority of what is traditionally considered to be social science research, this framework is effective and convenient—as long as the data size is smaller than the available memory, and the software is capable of interpreting it, researchers can continue to look towards traditional frameworks for statistical analysis. When data size outgrows this defined capacity level, however, relying on this framework results in inefficiencies and/or prohibitive costs for most organizations. In some cases, it is possible to work with data of greater size than that of your system memory by treating physical storage as “virtual” memory, but this easily results in protracted processing times and is unadvisable. An alternative approach is to acquire a larger server – this approach, however, is also not appropriate for most researchers working with massive data. While a larger server would allow researchers to work with more data, the size of the data that can be stored and analyzed on that server is still finite. Consequently, researchers experience a significant increase in computing cost by housing and maintaining a server that they may not continuously leverage or that may not be large enough to store some data sets (or have enough memory to complete certain processes). This is particularly true if multiple researchers that work with massive data share a server. Subsequent sections of this manual describe the solution developed by UI Research Programming to address the needs of policy researchers that work with massive data. The system described below does not require UI to own and maintain a server capable of handling massive data. Our approach instead leverages scalable cloud computing hosted by Amazon Web Services (AWS) and the distributed computing platform Apache Spark (Spark). The framework is directly tied to the elasticity of researcher demand for computational capacity, simultaneously providing researchers with greater flexibility at reduced organizational costs. When should researchers make the jump from the server to distributed computing? The answer depends on the size of the researcher’s data, the size of the server that the researcher currently relies on and the number of researchers sharing that server. Researchers should utilize distributed computing if their data is larger in size than the server memory and storage available to them (after sharing server resources with other researchers). 1.2 Making distributed computing straightforward &amp; cost-effective The solution developed by Research Programming to address massive data needs relies on cloud-based, distributed computing rather than an on-site server. The Spark distributed computing platform stores and processes data across a cluster of machines, distributing the data storage and processing tasks across the cluster. The cloud computing services hosted by AWS allows researchers to “spin-up” and shut down groupings of virtual servers, called Elastic Compute Cloud (EC2) instances, as needed for analysis while permanently storing data in the AWS data storage infrastructure, S3. The instance types available to researchers are determined by combinations of memory and storage and networking capacity,1 and researchers are encouraged to consult with Research Programming in order to identify what instance type is most appropriate for their project. This cloud-based, distributed framework effectively removes any computing constraint on the size of data that researchers can store and analyze. Researchers can rent any number of machines from AWS and then use Spark, which coordinates tasks between machines, to implement their data analysis and manipulation. Data is stored in S3, and then distributed in memory across the cluster machines when tasks need to be performed. This can be scaled to a massive degree – the largest known cluster to have utilized Spark is 8,000 machines and Spark has been shown to perform well processing up to several petabytes (each 1 million gigabytes) worth of data.2 Installation of Spark occurs during AWS cluster configuration so that researchers can immediately perform work with their data in the distributed environment, accessing Spark through a common programming language of their choice, once a cluster is finished spinning up. We describe and compare supported programming languages in subsequent sections of this manual. A complete list of EC2 instance types can be found at https://aws.amazon.com/ec2/instance-types/.↩ http://spark.apache.org/faq.html↩ "],
["getting-started-with-spark.html", "Chapter 2 - Getting Started with Spark 2.1 What About Stata and SAS? 2.2 Choosing Between SparkR and PySpark", " Chapter 2 - Getting Started with Spark 2.1 What About Stata and SAS? Unfortunately, neither Stata nor SAS are supported by Spark. The framework developed by Research Programming leverages PySpark and SparkR (the Spark forms of Python and R, respectively), enabling researchers to perform Spark operations using either language. Below are links to tutorials written by Research Programming to help guide researchers through tasks typically performed by social scientists, but driven by Spark: PySpark (Python): https://github.com/UrbanInstitute/pyspark-tutorials SparkR (R): https://github.com/UrbanInstitute/sparkr-tutorials Note: The tutorials linked above assume that readers have some familiarity with Python and R, respectively. Please, reach out to Research Programming if you’re interested in working with a massive data set, but are not familiar with Python or R. 2.2 Choosing Between SparkR and PySpark If you have a strong preference of language for R or Python, you should let that preference guide your decision. SparkR and PySpark are not dramatically different in terms of speed or available functionality (though we have learned that generalized linear modeling is more intuitive in SparkR). Speed: Each of the Spark language implementations (R, Python, Java and Scala) communicates with the Spark data type DataFrame, which is conceptually equivalent to a table in a relational database or a data.frame/DataFrame in R/Python. Spark DataFrames use Scala and Java (languages that compose core Spark) to optimize operations so that the execution speeds for standard data processing tasks written in any of these languages are functionally the same. Available Functionality: There is some difference in Spark 1.6 between SparkR and PySpark concerning the availability of core Spark functionality, with PySpark having greater functionality. However, Spark 2.0 significantly reduces the functionality gap between the two. That being said, there are some notable differences that might affect your decision: Statistical Modeling: We have found SparkR to natively support statistical modeling with far more ease the PySpark. R’s normal formula syntax works well, whereas PySpark’s implementation has added complexity. If easy and familiar statistical modeling is important, you might want to stick with SparkR. Code Syntax: SparkR feels a bit more like normal R than PySpark feels like Python, although both have peculiarities. This is perhaps the case since R is natively built around data.frames, which are analogous to Spark DataFrames. For this reason, you might prefer SparkR if you’re equally competent at R and Python. Both of the tutorials linked above are oriented around the Spark DataFrame data type. Open Source Development: It seems like Python has the advantage here, in that more of the packages extending the functionality come written in or are accessible through PySpark than SparkR. This is likely a product of the current Spark community – more data scientists and engineers than traditional statisticians and social scientists. "],
["understanding-spark.html", "Chapter 3 - Understanding Spark 3.1 What Makes Distributed Processing Different?", " Chapter 3 - Understanding Spark 3.1 What Makes Distributed Processing Different? The analysis that researchers can perform, and the ways in which that analysis is carried out, can change significantly when data is too large to be loaded onto a single machine. As a simple example, let’s examine how one would calculate the standard deviation of a numerical variable in a distributed environment. We have one column of data with 22 rows of values. Assume for simplicity that this data is too large to fit onto a single machine and is, therefore, distributed across five machines as follows: Machine 1 Machine 2 Machine 3 Machine 4 Machine 5 \\[15\\] \\[23\\] \\[5\\] \\[33\\] \\[1\\] \\[6\\] \\[7\\] \\[4\\] \\[27\\] \\[0\\] \\[9\\] \\[2\\] \\[6\\] \\[20\\] \\[19\\] \\[10\\] \\[6\\] \\[11\\] \\[10\\] \\[17\\] \\[9\\] \\[10\\] Spark begins calculating the standard deviation of the variable by directing each machine to calculate both the sum of the variable and the number of observations across each machine: Machine 1 Machine 2 Machine 3 Machine 4 Machine 5 Sum \\[40\\] \\[55\\] \\[15\\] \\[100\\] \\[40\\] Count \\[4\\] \\[5\\] \\[3\\] \\[5\\] \\[5\\] Each machine then returns its sum and count to the central manager (called the “master”). The reduced data described in the table above is sufficiently small to fit onto the single master machine. Even a massive cluster with 8,000 machines would aggregate only 16,000 values, a number which a single machine could easily hold. The master then uses the reduced data to calculate and return the mean for the entire column (equal to 11.36) to each machine, which then square the difference for each value: Machine 1 Machine 2 Machine 3 Machine 4 Machine 5 \\[(15-11.36)^2\\] \\[(23-11.36)^2\\] \\[(5-11.36)^2\\] \\[(33-11.36)^2\\] \\[(1-11.36)^2\\] \\[(6-11.36)^2\\] \\[(7-11.36)^2\\] \\[(4-11.36)^2\\] \\[(27-11.36)^2\\] \\[(0-11.36)^2\\] \\[(9-11.36)^2\\] \\[(2-11.36)^2\\] \\[(6-11.36)^2\\] \\[(20-11.36)^2\\] \\[(19-11.36)^2\\] \\[(10-11.36)^2\\] \\[(6-11.36)^2\\] \\[(11-11.36)^2\\] \\[(10-11.36)^2\\] \\[(17-11.36)^2\\] \\[(9-11.36)^2\\] \\[(10-11.36)^2\\] After this there is another sum and count operation that is returned to the master machine: Machine 1 Machine 2 Machine 3 Machine 4 Machine 5 \\[13.2496\\] \\[135.4896\\] \\[40.4496\\] \\[468.2896\\] \\[107.3296\\] \\[28.7296\\] \\[19.0096\\] \\[54.1696\\] \\[244.6096\\] \\[129.0496\\] \\[5.5696\\] \\[87.6096\\] \\[28.7296\\] \\[74.6496\\] \\[58.3696\\] \\[1.8496\\] \\[28.7296\\] \\[0.1296\\] \\[1.8496\\] \\[31.8096\\] \\[5.5696\\] \\[1.8496\\] Sum \\[49.3984\\] \\[302.648\\] \\[123.3488\\] \\[793.248\\] \\[298.448\\] Count \\[4\\] \\[5\\] \\[3\\] \\[5\\] \\[5\\] The master machine then uses these values to calculate the mean again, followed by the square root, resulting in the standard deviation of 8.44. While computing the standard deviation of a numerical variable is a relatively trivial operation for Spark to complete, other, more involved operations clearly could become difficult or even impossible to complete in a distributed environment.3 This is a fairly simple illustration of how Spark generally works behind the scenes: some command is sent to each machine to map to the data it holds, then the results are reduced and returned to the master machine. In fact, the aptly-named predecessor to Spark is called MapReduce.↩ "],
["sorting-distributed-data.html", "Chapter 4 - Sorting Distributed Data", " Chapter 4 - Sorting Distributed Data When working with datasets of sizes traditionally seen in social science research, sorting the data by some value rule is a relatively easy task. When all of the data is in the memory of one system, sorting rows is close to trivial. Massive data in a distributed environment, in which perhaps many millions of observations are distributed across hundreds of machines, sorting that data becomes an enormously expensive operation. Spark needs to first consider your sorting criteria (e.g. dates, alphabetical, numerical, ascending, descending), then search through all of the systems in the cluster to figure out what and where the first and last values are, what the intervals are between values and whether there are duplicates or not. From there it has to estimate where all of the data should go and where to shift the values of which this data will take the spot. If January is located next to December in the ranking of month values and Spark finds that February should be in that ranking, for example, it needs to determine where to shift December before the task can be completed. Each observation takes up space in memory: Spark cannot simply hold every observation in a single machine until it identifies where a particular observations should be placed. The data, therefore, becomes more spread out when information is “shuffled” since Spark holds observations elsewhere until it determines where they should be placed. Additionally, when the data is on multiple machines, shuffling can force data onto the hard drive in the course of it being shifted by Spark. Hard drive traffic is much slower than simply moving data around within memory, as might happen when you sort an Excel column. Any operation that causes a shuffling of data, or a “shuffle,” can be very costly in terms of processing power, hard drive traffic and system memory. Researchers should understand that several tasks that can be performed in Spark require a shuffle, and that this shuffling may be obscured. Spark operations that do require a shuffle include, for example, are joining datasets on equal values of some specified column and finding unique values of a column. Some shuffling is often inevitable; just note that shuffles will be one of the largest drivers of how long your operations take to finish. "],
["econometrics-and-large-scale-data.html", "Chapter 5 - Econometrics and Large-scale Data 5.1 Implications for Hypothesis Testing 5.2 Computational Implications", " Chapter 5 - Econometrics and Large-scale Data 5.1 Implications for Hypothesis Testing Policy researchers making frequentist inferences beware: when performing hypothesis tests on large-scale data, reliance on statistical significance for making meaningful assertions about the outcomes of hypothesis tests will often fail you. Any parameter that is not exactly equal to the null hypothesis value will become statistically significant as sample size (n) increases infinitely, and with p-values that are much smaller in size than thresholds commonly used in social science (e.g. \\[p &lt; 0.01\\]**). Note that, if the assumptions of a test are true and the true parameter is exactly equal to the null value, then a large sample size will not lead to a rejection of the null hypothesis. In practice, however, we can easily extract statistically significant parameter estimates from any data with a size that would require the use of Spark. A true parameter value is very unlikely to be exactly equal to a specified null in reality: consequently, an extremely large sample size can increase the magnitude of the test statistic, and drive down the p-value far past standard thresholds of statistical significance.4 So, a parameter estimate that is arbitrarily close to the null value that is not statistically significant at a standard sample size (e.g. n = 15,000) may become statistically significant if the number of observations drawn from the same population increased to several hundred million. While the estimate would be statistically significant, the parameter estimate would be so close in value to the null that the outcome may be uninteresting. If arbitrarily small p-values are abundant, finding p-values that are smaller in size than traditional cutoffs for significance cannot be used as compelling evidence in building an empirical argument,5 and policy researchers must employ alternative heuristics for making policy inferences in this context. 5.1.1 Statistical v. Practical Significance One conceptual shift that many researchers will need to make when working with massive data is that inference is no longer primarily about detecting some effect, but is rather focused on examining the data for evidence of an absence of an effect. In particular, the abundance of arbitrarily small p-values in massive data analysis requires that researchers give greater importance to identifying whether an estimate is practically significant rather than its likely statistical significance (since it is likely that the majority of estimates will be statistically significant). One of the most important indicators of practical significance is effect size (e.g. correlation, regression coefficient or mean difference across groups) and, in conjunction, confidence intervals. Remember that the range of a confidence interval is determined by the critical value for the t-distribution, the standard deviation estimate and sample size. Reporting the effect size of a parameter through confidence intervals: Provides a more precise description of the magnitude and direction of an effect relative to reporting only the parameter estimate and, more generally, and Describes the confidence of the parameter estimate being true rather than rejecting (or failing to reject) a null hypothesis. Although the large sample size can cause the range of the confidence intervals to be extremely small, using statements of confidence to examine the importance of a parameter estimate is a useful strategy when implementing a hypothesis test that is very sensitive (e.g. the simple \\[H_0\\]: \\[\\theta = 0\\] hypothesis test). Note that this approach requires researchers to make some a priori decision about what constitutes an practically significant value in the context of their research question. 5.2 Computational Implications In addition to the theoretical implications of working with massive data discussed above, analyzing data with Spark also has computational implications that policy researchers should be aware of. Spark fits linear models using iterative optimization methods that are well-suited for large-scale and distributed computation. Specifically, these methods use iterative algorithms to minimize the cost function of the linear model. Currently, the SparkR operation spark.glm uses only iteratively reweighted least squares IRLS to fit a linear model. This “solver” can fit a model with a maximum number of 4096 model features. PySpark similarly fits linear models using IRLS with the GeneralizedLinearRegression operation, which also can interpret up to 4096 feature models. If needed, there are PySpark operations that allow researchers to fit linear models with more than 4096: LinearRegression and LogisticRegression fit linear models using the Limited-memory Broyden–Fletcher–Goldfarb–Shanno L-BFGS algorithm. L-BFGS approximates a local minimum of the cost function of a model. However, as long as the cost function of the model is convex, the function will have only one global minimum and no other local minima—remember that the cost function for a linear regression model is a convex quadratic function. Therefore, L-BFGS will consistently compute parameter estimates for standard social science models. In the appendix, we provide a sketch of a proof for the claim that some mean parameter estimate of a random variable will become trivially statistically significant as n goes to infinity (if the true value is not exactly equal to the null). We can similarly show that differences in mean parameter estimates, regression coefficient estimates and linear combinations of coefficient estimates behave this way under unbounded n.↩ While small p-values are not sufficient for making meaningful inferences, they are necessary for making inferences with statistical confidence. If a hypothesis test results in a small test statistic and, therefore, a large p-value, then: the sample is still not sufficiently large to accurately estimate the parameter, we have estimated the parameter with too little precision or the true parameter is arbitrarily close to the null or some combination of listed reasons.↩ "],
["econometrics-and-large-scale-data-1.html", "Chapter 6 - Econometrics and Large-scale Data 6.1 Implications for Hypothesis Testing 6.2 Computational Implications", " Chapter 6 - Econometrics and Large-scale Data 6.1 Implications for Hypothesis Testing Policy researchers making frequentist inferences beware: when performing hypothesis tests on large-scale data, reliance on statistical significance for making meaningful assertions about the outcomes of hypothesis tests will often fail you. Any parameter that is not exactly equal to the null hypothesis value will become statistically significant as sample size, \\(n\\), increases infinitely, and with p-values that are much smaller in size than thresholds commonly used in social science (e.g. \\(p &lt; 0.01\\)**). Note that, if the assumptions of a test are true and the true parameter is exactly equal to the null value, then a large sample size will not lead to a rejection of the null hypothesis. In practice, however, we can easily extract statistically significant parameter estimates from any data with a size that would require the use of Spark. A true parameter value is very unlikely to be exactly equal to a specified null in reality: consequently, an extremely large sample size can increase the magnitude of the test statistic, and drive down the p-value far past standard thresholds of statistical significance.6 So, a parameter estimate that is arbitrarily close to the null value that is not statistically significant at a standard sample size (e.g. \\(n = 15,000\\)) may become statistically significant if the number of observations drawn from the same population increased to several hundred million. While the estimate would be statistically significant, the parameter estimate would be so close in value to the null that the outcome may be uninteresting. If arbitrarily small p-values are abundant, finding p-values that are smaller in size than traditional cutoffs for significance cannot be used as compelling evidence in building an empirical argument,7 and policy researchers must employ alternative heuristics for making policy inferences in this context. 6.1.1 Statistical v. Practical Significance One conceptual shift that many researchers will need to make when working with massive data is that inference is no longer primarily about detecting some effect, but is rather focused on examining the data for evidence of an absence of an effect. In particular, the abundance of arbitrarily small p-values in massive data analysis requires that researchers give greater importance to identifying whether an estimate is practically significant rather than its likely statistical significance (since it is likely that the majority of estimates will be statistically significant). One of the most important indicators of practical significance is effect size (e.g. correlation, regression coefficient or mean difference across groups) and, in conjunction, confidence intervals. Remember that the range of a confidence interval is determined by the critical value for the t-distribution, the standard deviation estimate and sample size. Reporting the effect size of a parameter through confidence intervals: Provides a more precise description of the magnitude and direction of an effect relative to reporting only the parameter estimate and, more generally, and Describes the confidence of the parameter estimate being true rather than rejecting (or failing to reject) a null hypothesis. Although the large sample size can cause the range of the confidence intervals to be extremely small, using statements of confidence to examine the importance of a parameter estimate is a useful strategy when implementing a hypothesis test that is very sensitive (e.g. the simple \\(H_0\\): \\(\\theta = 0\\) hypothesis test). Note that this approach requires researchers to make some a priori decision about what constitutes an practically significant value in the context of their research question. 6.2 Computational Implications In addition to the theoretical implications of working with massive data discussed above, analyzing data with Spark also has computational implications that policy researchers should be aware of. Spark fits linear models using iterative optimization methods that are well-suited for large-scale and distributed computation. Specifically, these methods use iterative algorithms to minimize the cost function of the linear model. Currently, the SparkR operation spark.glm uses only iteratively reweighted least squares IRLS to fit a linear model. This “solver” can fit a model with a maximum number of 4096 model features. PySpark similarly fits linear models using IRLS with the GeneralizedLinearRegression operation, which also can interpret up to 4096 feature models. If needed, there are PySpark operations that allow researchers to fit linear models with more than 4096: LinearRegression and LogisticRegression fit linear models using the Limited-memory Broyden–Fletcher–Goldfarb–Shanno L-BFGS algorithm. L-BFGS approximates a local minimum of the cost function of a model. However, as long as the cost function of the model is convex, the function will have only one global minimum and no other local minima—remember that the cost function for a linear regression model is a convex quadratic function. Therefore, L-BFGS will consistently compute parameter estimates for standard social science models. In the appendix, we provide a sketch of a proof for the claim that some mean parameter estimate of a random variable will become trivially statistically significant as n goes to infinity (if the true value is not exactly equal to the null). We can similarly show that differences in mean parameter estimates, regression coefficient estimates and linear combinations of coefficient estimates behave this way under unbounded \\(n\\).↩ While small p-values are not sufficient for making meaningful inferences, they are necessary for making inferences with statistical confidence. If a hypothesis test results in a small test statistic and, therefore, a large p-value, then: the sample is still not sufficiently large to accurately estimate the parameter, we have estimated the parameter with too little precision or the true parameter is arbitrarily close to the null or some combination of listed reasons.↩ "],
["appendix.html", "Chapter 7 - Appendix 7.1 Unbounded sample size &amp; statistical significance – sketch of proof", " Chapter 7 - Appendix 7.1 Unbounded sample size &amp; statistical significance – sketch of proof Let the sample mean, \\(\\hat{\\mu}\\), be the parameter estimate for our mean parameter \\(\\mu\\) and the null hypothesis of the t-test be \\(H_0\\): \\(/mu = 0\\). The test statistic is given by \\(\\hat{\\mu} / (\\hat{\\sigma} / \\sqrt{n})\\). Remember that the p-value is determined by the test statistic and the t-distribution with \\((n – 2)\\) degrees of freedom in this case. By the Central Limit Theorem, \\(\\sqrt{n}*(\\hat{\\mu}-\\mu) \\rightarrow N(0,\\sigma^2)\\) as \\(n \\rightarrow \\infty\\), or written differently as \\(\\hat{\\mu} \\rightarrow \\mu + \\frac{\\sigma}{\\sqrt{n}}N(0,1)\\). Implicitly, \\(\\hat{\\mu} = \\mu + O(\\frac{1}{\\sqrt{n}})\\) and \\(\\hat{sigma} = \\sigma + O(\\frac{1}{\\sqrt{n}})\\). By substitution, \\[\\frac{\\hat{\\mu}}{\\hat{\\sigma}/\\sqrt{n}} = \\sqrt{n}\\frac{\\hat{\\mu}}{\\hat{\\sigma}}\\] \\[= \\sqrt{n}*\\frac{\\mu + O(n^{-1/2})}{\\sigma + O(n^{-1/2})}\\] \\[= \\sqrt{n}[\\frac{\\mu}{\\sigma}+O(n^{-1/2})]\\] \\[= \\sqrt{n}*\\frac{\\mu}{\\sigma}+O(1)\\] Therefore, the test statistic will approach positive or negative infinity at a constant rate of \\(\\sqrt{n}\\) the mean parameter does not exactly equal 0. As \\(n\\) approaches infinity and, therefore, the degrees of freedom \\((n – 2)\\) approach infinity, the t-distribution converges to a standard normal distribution. So, for any nonzero mean parameter, the p-value will always approach 0 as \\(n\\) goes to infinity. Similar arguments can be made for differences in means, regression coefficients or linear combinations of regression coefficients. For each parameter type, the p-value is similarly determined by the size of the parameter and the precision with which it is estimated, as well as the sample size. In each case, the p-value will converge stochastically to zero as \\(n\\) goes to infinity (unless the true parameter value is exactly equal to the null value). "]
]
